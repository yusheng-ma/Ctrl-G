{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fe4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "device = 'cuda'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # set your cuda device\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import torch\n",
    "import ctrlg\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList\n",
    "\n",
    "# load the pretrained base_model and hmm_model; see README.md for a complete list of \n",
    "# released checkpoints. note that the hmm_model and base_model must share the same \n",
    "# vocabulary of tokens: i.e., one cannot apply hmm_gpt2-large_common-gen_4096 to \n",
    "# tulu2-7b_writing-prompts. To apply Ctrl-G to a custom base_model or to achieve \n",
    "# best performance on a specific domain, users would need to distill an hmm_model\n",
    "# from the base_model. Please refer to tutorial_distillation.ipynb for details.\n",
    "BASE_MODEL_PATH = f'ctrlg/gpt2-large_common-gen' # a gpt2-large checkpoint domain adapted to the common-gen corpus\n",
    "HMM_MODEL_PATH = f'ctrlg/hmm_gpt2-large_common-gen_4096' # alternatively 'ctrlg/hmm_gpt2-large_common-gen_32768' for better quality\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH).to(device)\n",
    "base_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "hmm_model = ctrlg.HMM.from_pretrained(HMM_MODEL_PATH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950dcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = hmm_model.vocab_size\n",
    "eos_token_id = hmm_model.eos_token_id\n",
    "\n",
    "\n",
    "##################################### prefix, suffix, prompt #####################################\n",
    "prefix = '' # generate text starting with nothing\n",
    "suffix = '<|endoftext|>' # generate text ending with '<|endoftext|>'; a suffix must end with the eos token\n",
    "prompt = '<|endoftext|>' # prompt the base model with the '<|endoftext|>' token\n",
    "\n",
    "prefix_ids = tokenizer.encode(prefix)\n",
    "suffix_ids = tokenizer.encode(suffix)\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "##################################### prefix, suffix, prompt #####################################\n",
    "\n",
    "\n",
    "##################################### DFA Construction #####################################\n",
    "my_flat_json_builder = ctrlg.myFlatJsonBuilder(tokenizer, vocab_size)\n",
    "\n",
    "dfa_graphs = []\n",
    "\n",
    "dfa_graphs.append(my_flat_json_builder.build())\n",
    "\n",
    "# taking the intersection of the DFAs, i.e., \"logical and\" of the constraints.\n",
    "# This function also minimizes the constructed DFA, which is mainly CPU-based operations;\n",
    "# Due to its pure python implemenation, DFA minimization can be slow for complex constraints\n",
    "dfa_graph = ctrlg.DFA_prod(dfa_graphs, mode='intersection')\n",
    "\n",
    "# compile the dfa_graph for efficient GPU execution\n",
    "dfa_model = ctrlg.DFAModel(dfa_graph, vocab_size).to(device)\n",
    "##################################### DFA Construction #####################################\n",
    "\n",
    "\n",
    "##################################### token length #####################################\n",
    "# specify the min_new_tokens and max_new_tokens to be generated (excluding\n",
    "# the prefix and suffix) make sure that the numbers here would not conflict\n",
    "# with the given constraint: e.g. ask the model to generate 10 words with\n",
    "# max_new_tokens = 8\n",
    "min_new_tokens = 5\n",
    "max_new_tokens = 32\n",
    "##################################### token length #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5af064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialze the constraints logits processor\n",
    "# Note: this part pre-computes & cache certain conditional probability tables;\n",
    "# one simple optimization is to re-use the same constraint_logits_processor for\n",
    "# base_model.generate if the constraints do not change.\n",
    "constraint_logits_processor = ctrlg.ConstraintLogitsProcessor(\n",
    "    hmm_model, dfa_model,\n",
    "    min_new_tokens, max_new_tokens,\n",
    "    prompt_ids, prefix_ids=prefix_ids, suffix_ids=suffix_ids)\n",
    "\n",
    "\n",
    "# set beam_size for beam search; usually the larger the beam_size the\n",
    "# higher the generation quality\n",
    "beam_size = 128\n",
    "\n",
    "# set the hmm_batch_size depending on the resource available;\n",
    "# uses more memory with larger hmm_batch_size but attains best speed \n",
    "# when it is set to beam_size\n",
    "constraint_logits_processor.hmm_batch_size = beam_size\n",
    "\n",
    "# generate with beam search\n",
    "input_ids = torch.tensor([prompt_ids], device=device)\n",
    "outputs = base_model.generate(\n",
    "        input_ids=input_ids, do_sample=False, length_penalty=0.2,\n",
    "        num_beams=beam_size, num_return_sequences=beam_size,\n",
    "        min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "        logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d5d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. \u001b[1m { \"color\" : \"red\" }\u001b[0m\n",
      "1. \u001b[1m { \"color\" : \"green\" }\u001b[0m\n",
      "2. \u001b[1m { \"color\" : \"white\" }\u001b[0m\n",
      "3. \u001b[1m { \"color\" : \"black\" }\u001b[0m\n",
      "4. \u001b[1m { \"position\" : \"relative\" }\u001b[0m\n",
      "5. \u001b[1m{ \"color\" : \"red\" }\u001b[0m\n",
      "6. \u001b[1m { \"size\" : \"medium\" }\u001b[0m\n",
      "7. \u001b[1m { \"direction\" : \"north\" }\u001b[0m\n",
      "8. \u001b[1m { \"color\" : \"blue\" }\u001b[0m\n",
      "9. \u001b[1m { \"color\" : \"grey\" }\u001b[0m\n",
      "10. \u001b[1m { \"color\" : \"orange\" }\u001b[0m\n",
      "11. \u001b[1m { \"size\" : \"2\" }\u001b[0m\n",
      "12. \u001b[1m { \"direction\" : \"east\" }\u001b[0m\n",
      "13. \u001b[1m { \"size\" : \"1\" }\u001b[0m\n",
      "14. \u001b[1m { \"direction\" : \"west\" }\u001b[0m\n",
      "15. \u001b[1m { \"size\" : \"small\" }\u001b[0m\n",
      "16. \u001b[1m { \"color\" : \"gold\" }\u001b[0m\n",
      "17. \u001b[1m { \"direction\" : \"south\" }\u001b[0m\n",
      "18. \u001b[1m { \"position\" : \"absolute\" }\u001b[0m\n",
      "19. \u001b[1m { \"size\" : \"large\" }\u001b[0m\n",
      "20. \u001b[1m { \"animal\" : \"dog\" }\u001b[0m\n",
      "21. \u001b[1m{ \"position\" : \"relative\" }\u001b[0m\n",
      "22. \u001b[1m { \"style\" : \"text\" }\u001b[0m\n",
      "23. \u001b[1m { \" style \" : \" color \" }\u001b[0m\n",
      "24. \u001b[1m { \"name\" : \"name\" }\u001b[0m\n",
      "25. \u001b[1m { \"container\" : \"container\" }\u001b[0m\n",
      "26. \u001b[1m { \"type\" : \"text\" }\u001b[0m\n",
      "27. \u001b[1m { \"type\" : \"cube\" }\u001b[0m\n",
      "28. \u001b[1m { \"style\" : \"grid\" }\u001b[0m\n",
      "29. \u001b[1m { \"family\" : \"family\" }\u001b[0m\n",
      "30. \u001b[1m { \"image\" : \"red\" }\u001b[0m\n",
      "31. \u001b[1m { \" style \" : \" text \" }\u001b[0m\n",
      "32. \u001b[1m { \"style\" : \"color\" }\u001b[0m\n",
      "33. \u001b[1m { \" type \" : \" object \" }\u001b[0m\n",
      "34. \u001b[1m { \"image\" : \"small\" }\u001b[0m\n",
      "35. \u001b[1m { \"type\" : \"food\" }\u001b[0m\n",
      "36. \u001b[1m { \"type\" : \"string\" }\u001b[0m\n",
      "37. \u001b[1m { \"type\" : \"table\" }\u001b[0m\n",
      "38. \u001b[1m { \"type\" : \"image\" }\u001b[0m\n",
      "39. \u001b[1m { \"type\" : \"city\" }\u001b[0m\n",
      "40. \u001b[1m { \"default\" : \"true\" }\u001b[0m\n",
      "41. \u001b[1m { \"type\" : \"artist\" }\u001b[0m\n",
      "42. \u001b[1m { \" style \" : \" font \" }\u001b[0m\n",
      "43. \u001b[1m { \" type \" : \" text \" }\u001b[0m\n",
      "44. \u001b[1m { \"type\" : \"event\" }\u001b[0m\n",
      "45. \u001b[1m { \"float\" : \"true\" }\u001b[0m\n",
      "46. \u001b[1m { \"style\" : \"background\" }\u001b[0m\n",
      "47. \u001b[1m { \"name\" : \"home\" }\u001b[0m\n",
      "48. \u001b[1m{ \"position\" : \"absolute\" }\u001b[0m\n",
      "49. \u001b[1m { \"type\" : \"collection\" }\u001b[0m\n",
      "50. \u001b[1m { \"type\" : \"file\" }\u001b[0m\n",
      "51. \u001b[1m { \"type\" : \"object\" }\u001b[0m\n",
      "52. \u001b[1m { \" style \" : \" style \" }\u001b[0m\n",
      "53. \u001b[1m { \"name\" : \"test\" }\u001b[0m\n",
      "54. \u001b[1m { \"title\" : \"test\" }\u001b[0m\n",
      "55. \u001b[1m { \"style\" : \"font\" }\u001b[0m\n",
      "56. \u001b[1m { \"type\" : \"directory\" }\u001b[0m\n",
      "57. \u001b[1m { \"title\" : \"Test\" }\u001b[0m\n",
      "58. \u001b[1m { \" style \" : \" background \" }\u001b[0m\n",
      "59. \u001b[1m { \"type\" : \"block\" }\u001b[0m\n",
      "60. \u001b[1m { \"name\" : \"family\" }\u001b[0m\n",
      "61. \u001b[1m { \"name\" : \"blue\" }\u001b[0m\n",
      "62. \u001b[1m { \"name\" : \"color\" }\u001b[0m\n",
      "63. \u001b[1m { \"title\" : \"Home\" }\u001b[0m\n",
      "64. \u001b[1m { \"image\" : \"img\" }\u001b[0m\n",
      "65. \u001b[1m { \"grid\" : \"grid\" }\u001b[0m\n",
      "66. \u001b[1m { \"name\" : \"green\" }\u001b[0m\n",
      "67. \u001b[1m { \"event\" : \"event\" }\u001b[0m\n",
      "68. \u001b[1m { \"bright\" : \"true\" }\u001b[0m\n",
      "69. \u001b[1m { \"episode\" : \"episode\" }\u001b[0m\n",
      "70. \u001b[1m { \"color\" : \"dark\" }\u001b[0m\n",
      "71. \u001b[1m { \"name\" : \"red\" }\u001b[0m\n",
      "72. \u001b[1m { \"date\" : \"2010\" }\u001b[0m\n",
      "73. \u001b[1m { \"name\" : \"A\" }\u001b[0m\n",
      "74. \u001b[1m { \"hello\" : \"hello\" }\u001b[0m\n",
      "75. \u001b[1m { \"name\" : \"a\" }\u001b[0m\n",
      "76. \u001b[1m { \"type\" : \"question\" }\u001b[0m\n",
      "77. \u001b[1m { \"image\" : \"http\" }\u001b[0m\n",
      "78. \u001b[1m { \"name\" : \"new\" }\u001b[0m\n",
      "79. \u001b[1m { \"name\" : \"Red\" }\u001b[0m\n",
      "80. \u001b[1m { \"name\" : \"the\" }\u001b[0m\n",
      "81. \u001b[1m { \"type\" : \"category\" }\u001b[0m\n",
      "82. \u001b[1m { \"title\" : \"App\" }\u001b[0m\n",
      "83. \u001b[1m { \"title\" : \"A\" }\u001b[0m\n",
      "84. \u001b[1m { \"genre\" : \"music\" }\u001b[0m\n",
      "85. \u001b[1m { \"name\" : \"App\" }\u001b[0m\n",
      "86. \u001b[1m { \"name\" : \"S\" }\u001b[0m\n",
      "87. \u001b[1m { \"title\" : \"The \" }\u001b[0m\n",
      "88. \u001b[1m { \"title\" : \"S\" }\u001b[0m\n",
      "89. \u001b[1m { \"name\" : \"C\" }\u001b[0m\n",
      "90. \u001b[1m { \"image\" : \"https\" }\u001b[0m\n",
      "91. \u001b[1m { \"genre\" : \"rock\" }\u001b[0m\n",
      "92. \u001b[1m { \"title\" : \"E\" }\u001b[0m\n",
      "93. \u001b[1m { \"title\" : \"C\" }\u001b[0m\n",
      "94. \u001b[1m { \"name\" : \"E\" }\u001b[0m\n",
      "95. \u001b[1m { \"name\" : \"s\" }\u001b[0m\n",
      "96. \u001b[1m { \"genre\" : \"fiction\" }\u001b[0m\n",
      "97. \u001b[1m { \"name\" : \"My\" }\u001b[0m\n",
      "98. \u001b[1m { \"name\" : \"O\" }\u001b[0m\n",
      "99. \u001b[1m { \"title\" : \"My \" }\u001b[0m\n",
      "100. \u001b[1m { \"image\" : \"\\/\\/\" }\u001b[0m\n",
      "101. \u001b[1m { \"question\" : \"What\" }\u001b[0m\n",
      "102. \u001b[1m { \"question\" : \"Which\" }\u001b[0m\n",
      "103. \u001b[1m { \"catentry\" : \"10\" }\u001b[0m\n",
      "104. \u001b[1m { \"question\" : \"What \" }\u001b[0m\n",
      "105. \u001b[1m { \"sequence\" : \"10\" }\u001b[0m\n",
      "106. \u001b[1m { \"question\" : \"Which \" }\u001b[0m\n",
      "107. \u001b[1m { \"question\" : \"How \" }\u001b[0m\n",
      "108. \u001b[1m { \"question\" : \"Is \" }\u001b[0m\n",
      "109. \u001b[1m { \"catentry\" : \"12\" }\u001b[0m\n",
      "110. \u001b[1m { \"question\" : \"Which \"}\u001b[0m\n",
      "111. \u001b[1m { \"catentry\" : \"true\" }\u001b[0m\n",
      "112. \u001b[1m { \"catentry\" : \"102\" }\u001b[0m\n",
      "113. \u001b[1m { \"catentry\" : \"1080\" }\u001b[0m\n",
      "114. \u001b[1m { \"catentry\" : \"103\" }\u001b[0m\n",
      "115. \u001b[1m { \"catentry\" : \"1024\" }\u001b[0m\n",
      "116. \u001b[1m { \"catentry\" : \"108\" }\u001b[0m\n",
      "117. \u001b[1m { \"catentry\" : \"107\" }\u001b[0m\n",
      "118. \u001b[1m { \"catentry\" : \"101\" }\u001b[0m\n",
      "119. \u001b[1m { \"catentry\" : \"104\" }\u001b[0m\n",
      "120. \u001b[1m { \"catentry\" : \"BuyableInstoreAndOnline\" }\u001b[0m\n",
      "121. \u001b[1m { \"catentry\" : \"1027\" }\u001b[0m\n",
      "122. \u001b[1m { \"catentry\" : \"1027 \" }\u001b[0m\n",
      "123. \u001b[1m { \"catentry\" : \"101 \" }\u001b[0m\n",
      "124. \u001b[1m { \"catentry\" : \"109\" }\u001b[0m\n",
      "125. \u001b[1m { \"catentry\" : \"http\" }\u001b[0m\n",
      "126. \u001b[1m { \"catentry\" : \"catentry\" }\u001b[0m\n",
      "127. \u001b[1m { \"catentry\" : \"https\" }\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# extract the generated ids; removing prompt ids; remove suffix ids that are (partially) generated\n",
    "generated_ids = ctrlg.extract_generated_ids(outputs.tolist(), prompt_ids, suffix_ids, eos_token_id)\n",
    "\n",
    "# rank the generated ids by the base_model probability\n",
    "generated_ids = ctrlg.rank_generated_ids(base_model, generated_ids, prompt_ids, suffix_ids, length_penalty=0.2)\n",
    "\n",
    "# print top 10 outputs\n",
    "for idx, generated in enumerate(generated_ids):\n",
    "    print(f'{idx}. ' + tokenizer.decode(prefix_ids, skip_special_tokens=True) + \\\n",
    "          '\\033[1m' + tokenizer.decode(generated, skip_special_tokens=True) + '\\033[0m' + \\\n",
    "          tokenizer.decode(suffix_ids, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctrlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
